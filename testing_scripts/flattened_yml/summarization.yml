TASK : Seq2Seq_tasks
subtask : summarization
max_input_length : 512
max_target_length : 128
eval_metric : 'rouge' # bleu, rouge
cuda_id : '0'
OUTPUT_DIR : './results_txt'
OVERWRITE_OUTPUT_DIR : False
LOGGING_PATH: #'test_log.log'
packing : True
dataset_text_field : 'text'  #The name of the new field that is created after data loading
max_seq_length : 512
flash_attention2 : false
blocksize : 128
SAVE_METHOD : 'state_dict'


# DATASET_ARGS :
DATASET : 
DATA_VERSION : '1.0'
DATA_SPLIT : 
TRAIN_DIR : ''
VAL_DIR : ''
TEST_DIR : ''
MAX_TRAIN_SAMPLES : 1000
MAX_EVAL_SAMPLES : 1000
CUSTOM_DATASET_PATH : #'abc/dataset' 
DATASET_CONFIG : {}
input_column : 'document'
target_column : 'summary'
FORMAT_NAMES: #'document'  #Which columns are going to be used in the dataset (should be separated by ',')
FORMAT_STRING:  #The string according to which the dataset will be formatted


# DATASET_ARGS :
#   DATASET : 'xsum'
#   DATA_VERSION : '1.0'
#   DATA_SPLIT : 
#   TRAIN_DIR : ''
#   VAL_DIR : ''
#   TEST_DIR : ''
#   MAX_TRAIN_SAMPLES : 1000
#   MAX_EVAL_SAMPLES : 1000
#   CUSTOM_DATASET_PATH : 
#   DATASET_CONFIG : {}
#   input_column : 'document'
#   target_column : 'summary'
#   FORMAT_NAMES: 'document'  #Which columns are going to be used in the dataset (should be separated by ',')
#   FORMAT_STRING:  #The string according to which the dataset will be formatted



# DATASET_ARGS :
#   DATASET : 'xsum'
#   DATA_VERSION : '1.0'
#   DATA_SPLIT : 
#   TRAIN_DIR : ''
#   VAL_DIR : ''
#   TEST_DIR : ''
#   MAX_TRAIN_SAMPLES : 10
#   MAX_EVAL_SAMPLES : 10
#   CUSTOM_DATASET_PATH : 
#   DATASET_CONFIG : {}
#   input_column : 'document'
#   target_column : 'summary'

# MODEL_ARGS :
MODEL : #"t5"
MODEL_PATH :  #'t5-smalll'
MODEL_VERSION : '1.0'
CACHE_BOOL : False
MODEL_CHECKPOINT : 

# TRAINING_ARGS :
SEED : 56
DO_TRAIN : True
DO_EVAL : True
NUM_WORKERS : 4
BATCH_SIZE : 16
EPOCHS : 1
STEPS : 1
OPTIMIZER : 'adamw_torch'
LR : 1e-4
SCHEDULER_TYPE : 'linear'
WEIGHT_DECAY : 0.0
BETA1 : 0.9
BETA2 : 0.999
ADAM_EPS : 1e-8 
INTERVAL : 'epoch'
INTERVAL_STEPS : 100
NO_OF_CHECKPOINTS : 5
FP16 : False
RESUME_FROM_CHECKPOINT : False
GRADIENT_ACCUMULATION_STEPS : 1
GRADIENT_CHECKPOINTING : True
predict_with_generate: True
generation_max_length : 128
REMOVE_UNUSED_COLUMNS : True

# FINE_TUNING_ARGS :
LAST_LAYER_TUNING : False
FULL_FINE_TUNING : False

PEFT_METHOD : 

# LoRA_CONFIG :
r : 16
alpha : 8
dropout : 0.1
peft_type : #'LoRA'
target_modules : 
fan_in_fan_out : False
init_lora_weights : True  

# DoRA_CONFIG :
#   r : 16
#   alpha : 8
#   dropout : 0.1
#   peft_type : 'DoRA'
#   target_modules : 
#   fan_in_fan_out : False


# SSF_CONFIG :
#   peft_type : 'SSF'

# BNB_CONFIG :
# USE_4BIT :
load_in_4bit : False
bnb_4bit_compute_dtype : "float16"
bnb_4bit_quant_type : "nf4"
bnb_4bit_use_double_quant : False 
# USE_8BIT :
load_in_8bit : False  # only if model weights are in float16 or bfloat16
llm_int8_threshold : 6.0
llm_int8_skip_modules : 
llm_int8_enable_fp32_cpu_offload : False
llm_int8_has_fp16_weight : False

#DDP ARGS
DDP: False
num_nodes: 1
FSDP: False

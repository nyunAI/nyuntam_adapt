ADAM_EPS: 1e-8
BATCH_SIZE: 4
BETA1: 0.9
BETA2: 0.999
CACHE_BOOL: false
CUSTOM_DATASET_PATH: null
DATASET: mlabonne/guanaco-llama2-1k
DATASET_CONFIG: {}
DATASET_FORMAT: null
DATASET_ID: 27
DATASET_TYPE: LLM - Alpaca
DATA_SPLIT: 0.2
DATA_VERSION: '1.0'
DDP: false
DO_EVAL: false
DO_TRAIN: true
EPOCHS: 0.01
FILE_LOCATION: ''
FORMAT_NAMES: null
FORMAT_STRING: null
FP16: true
FSDP: true         #FSDP is set to true , accelerate_config will have the appropriate changes to run this via DDP
FULL_FINE_TUNING: false
GRADIENT_ACCUMULATION_STEPS: 1
GRADIENT_CHECKPOINTING: true
GROUP_BY_LENGTH: true
ID: 27
INTERVAL: steps
INTERVAL_STEPS: 50
JOB_ID: 100
JOB_SERVICE: Adapt
LAST_LAYER_TUNING: true
LOCAL_MODEL_PATH: null
LOGGING_PATH: /user_data/logs/Adapt/100
LR: 0.0002
Library: Huggingface
MAX_EVAL_SAMPLES: null
MAX_TRAIN_SAMPLES: null
MODEL: Llama-2
MODEL_CHECKPOINT: ''
MODEL_PATH: meta-llama/Meta-Llama-3-8B
MODEL_VERSION: '1.0'
NO_OF_CHECKPOINTS: 5
NUM_WORKERS: 4
OPTIMIZER: paged_adamw_32bit
OUTPUT_DIR: /user_data/jobs/Adapt/100
OVERWRITE_OUTPUT_DIR: false
PEFT_METHOD: LoRA
REMOVE_UNUSED_COLUMNS: true
RESUME_FROM_CHECKPOINT: false
SAVE_METHOD: state_dict
SCHEDULER_TYPE: constant
SEED: 56
STEPS: 100
TASK: text_generation
TEST_DIR: ''
TRAIN_DIR: ''
USER_FOLDER: /user_data
VAL_DIR: ''
WEIGHT_DECAY: 0.001
alpha: 16
blocksize: 128
bnb_4bit_compute_dtype: float16
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: false
cuda_id: '0'
dataset_text_field: text
dropout: 0.1
fan_in_fan_out: false
flash_attention2: false
init_lora_weights: true
llm_int8_enable_fp32_cpu_offload: false
llm_int8_has_fp16_weight: false
llm_int8_skip_modules: null
llm_int8_threshold: 6.0
load_in_4bit: true
load_in_8bit: false
max_seq_length: 512
num_nodes: 1
packing: true
peft_type: LoRA
r: 2
target_modules: null

TASK: 'text_classification'
subtask :  #'ner' # ner ,pos, chunk, #The dataset must contain columns - "ner_tags" , "pos_tags", "chunk_tags"
cuda_id : '0'
OUTPUT_DIR : './results_txt'
OVERWRITE_OUTPUT_DIR : False
LOGGING_PATH: '#test_log.log'
dataset_text_field : 'text'  #The name of the new field that is created after data loading
flash_attention2 : false
SAVE_METHOD : 'state_dict'

# DATASET_ARGS :
DATASET : 
DATA_VERSION : '1.0'
DATA_SPLIT : 
TRAIN_DIR : 
VAL_DIR : 
TEST_DIR : 
MAX_TRAIN_SAMPLES : 
MAX_EVAL_SAMPLES : 
CUSTOM_DATASET_PATH : #'abc/classification_data'
DATASET_CONFIG : {}
input_column : 'tokens'    #'text'    #The column which has the text in it
target_column : 'ner_tags'             #The column which has the labels in-case of text-classsification
FORMAT_NAMES:   #Which columns are going to be used in the dataset (should be separated by ',')
FORMAT_STRING:  #The string according to which the dataset will be formatted


  

# MODEL_ARGS :
MODEL : #"bert"
MODEL_PATH :  #'distilbert-base-uncased' 
MODEL_VERSION : '1.0'
CACHE_BOOL : False
MODEL_CHECKPOINT : 

# TRAINING_ARGS :
SEED : 56
DO_TRAIN : True
DO_EVAL : True
NUM_WORKERS : 4
BATCH_SIZE : 16
EPOCHS : 1
STEPS : 1
OPTIMIZER : 'adamw_torch' 
LR : 1e-4
SCHEDULER_TYPE : 'linear'
WEIGHT_DECAY : 0.0
BETA1 : 0.9
BETA2 : 0.999
ADAM_EPS : 1e-8 
INTERVAL : 'epoch'
INTERVAL_STEPS : 100
NO_OF_CHECKPOINTS : 5
FP16 : False
RESUME_FROM_CHECKPOINT : False
GRADIENT_ACCUMULATION_STEPS : 1
GRADIENT_CHECKPOINTING : False
REMOVE_UNUSED_COLUMNS : True

# FINE_TUNING_ARGS :
LAST_LAYER_TUNING : False
FULL_FINE_TUNING :  False


PEFT_METHOD : 

# LoRA_CONFIG :
r : 16
alpha : 8
dropout : 0.1
peft_type : #'LoRA'
target_modules : 
fan_in_fan_out : False
init_lora_weights : True  

# DoRA_CONFIG :
#   r : 16
#   alpha : 8
#   dropout : 0.1
#   peft_type : 'DoRA'
#   target_modules : 
#   fan_in_fan_out : False


# SSF_CONFIG :
#   peft_type : 'SSF'

# BNB_CONFIG :
# USE_4BIT :
load_in_4bit : False
bnb_4bit_compute_dtype : "float16"
bnb_4bit_quant_type : "nf4"
bnb_4bit_use_double_quant : False 
# USE_8BIT :
load_in_8bit : False  # only if model weights are in float16 or bfloat16
llm_int8_threshold : 6.0
llm_int8_skip_modules : 
llm_int8_enable_fp32_cpu_offload : False
llm_int8_has_fp16_weight : False


#DDP ARGS
DDP: False
num_nodes: 1
FSDP: False

TASK : Seq2Seq_tasks
subtask : translation
max_input_length : 128
max_target_length : 128
eval_metric : 'rouge' # sacrebleu, rouge
cuda_id : '0'
OUTPUT_DIR : './results_txt'
OVERWRITE_OUTPUT_DIR : False
source_lang : #'en'
target_lang : #'ru'
LOGGING_PATH: #'test_log.log'
dataset_text_field : 'text'  #The name of the new field that is created after data loading
PREFIX : #'translate English to Russian: '
flash_attention2 : False




# DATASET_ARGS :
DATASET : 
DATA_VERSION : '1.0'
DATA_SPLIT : 
TRAIN_DIR : ''
VAL_DIR : ''
TEST_DIR : ''
MAX_TRAIN_SAMPLES : 1000
MAX_EVAL_SAMPLES : 1000
CUSTOM_DATASET_PATH : #"abc/dataset"
DATASET_CONFIG : #"ru-en"
input_column : 
target_column : 
FORMAT_NAMES: #'translation'  #Which columns are going to be used in the dataset (should be separated by ',')
FORMAT_STRING:  #The string according to which the dataset will be formatted


# MODEL_ARGS :
MODEL : #"t5-base"
MODEL_PATH :  #'google-t5/t5-base'
MODEL_VERSION : '1.0'
CACHE_BOOL : False
MODEL_CHECKPOINT : 

# TRAINING_ARGS :
SEED : 56
DO_TRAIN : True
DO_EVAL :  True
NUM_WORKERS : 4
BATCH_SIZE : 160
EPOCHS : 1
STEPS : 1
OPTIMIZER : 'adamw_torch'
LR : 1e-4
SCHEDULER_TYPE : 'linear'
WEIGHT_DECAY : 0.0
BETA1 : 0.9
BETA2 : 0.999
ADAM_EPS : 1e-8 
INTERVAL : 'epoch'
INTERVAL_STEPS : 100
NO_OF_CHECKPOINTS : 5
FP16 : False
RESUME_FROM_CHECKPOINT : False
GRADIENT_ACCUMULATION_STEPS : 1
GRADIENT_CHECKPOINTING : True
predict_with_generate: True
generation_max_length : 128
REMOVE_UNUSED_COLUMNS : True


# FINE_TUNING_ARGS :
LAST_LAYER_TUNING : False
FULL_FINE_TUNING : False

PEFT_METHOD : 


# LoRA_CONFIG :
r : 16
alpha : 8
dropout : 0.1
peft_type : #'LoRA'
target_modules : 
fan_in_fan_out : False
init_lora_weights : True  

# DoRA_CONFIG :
#   r : 16
#   alpha : 8
#   dropout : 0.1
#   peft_type : 'DoRA'
#   target_modules : 
#   fan_in_fan_out : False

# SSF_CONFIG :
#   peft_type : 'SSF'

# BNB_CONFIG :
#   USE_4BIT :
load_in_4bit : False
bnb_4bit_compute_dtype : "float16"
bnb_4bit_quant_type : "nf4"
bnb_4bit_use_double_quant : False 
# USE_8BIT :
load_in_8bit : False  # only if model weights are in float16 or bfloat16
llm_int8_threshold : 6.0
llm_int8_skip_modules : 
llm_int8_enable_fp32_cpu_offload : False
llm_int8_has_fp16_weight : False

SAVE_METHOD : 'state_dict'


#DDP ARGS
DDP: False
num_nodes: 1
FSDP: False
